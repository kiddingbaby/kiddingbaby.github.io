---
title: "Ceph: 02-CephFS ä¸ NFS Gateway éƒ¨ç½²å®è·µ"
date: 2025-06-12T10:00:00+08:00
lastmod: 2025-06-12T10:00:00+08:00
categories: ["åŸºç¡€è®¾æ–½"]
tags: ["CephFS", "NFS", "åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ", "é«˜å¯ç”¨", "ä¼ä¸šå®è·µ"]
series: ["Ceph"]
keywords: ["CephFS éƒ¨ç½²", "NFS Ganesha", "åˆ†å¸ƒå¼æ–‡ä»¶å­˜å‚¨"]
ShowToc: true
TocOpen: true
draft: true
---

## CephFS æ˜¯ä»€ä¹ˆ

æœ¬æ–‡æ¥ä¸Šç¯‡ [åŸºäº cephadm éƒ¨ç½²é«˜å¯ç”¨ Ceph å­˜å‚¨é›†ç¾¤]({{< relref "posts/infra/storage/01-cephadm.md" >}})ã€‚

Ceph æ–‡ä»¶ç³»ç»Ÿ ( CephFS ) æ˜¯ä¸€ä¸ªç¬¦åˆ POSIX æ ‡å‡†çš„æ–‡ä»¶ç³»ç»Ÿï¼Œå®ƒæ„å»ºäº Ceph çš„åˆ†å¸ƒå¼å¯¹è±¡å­˜å‚¨RADOSä¹‹ä¸Šã€‚

![CephFS æ¶æ„å›¾](/images/posts/cephfs-architecture.svg)

ä¹‹å‰æˆ‘ä»¬å·²ç»éƒ¨ç½²å¥½äº†ä¸€ä¸ªç®€å•çš„ Ceph é›†ç¾¤ï¼Œå¹¶æ·»åŠ äº† OSDã€‚è¯ä¸å¤šè¯´ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬ç»§ç»­ã€‚

## éƒ¨ç½² CephFS

è¦ä½¿ç”¨ CephFS æ–‡ä»¶ç³»ç»Ÿï¼Œéœ€è¦ä¸€ä¸ªæˆ–å¤šä¸ª MDSï¼ˆMetadata Serverï¼‰å®ˆæŠ¤è¿›ç¨‹ï¼Œè¿™é‡Œä½¿ç”¨è¾ƒæ–°çš„ `ceph fs volume` æ¥å£æ¥åˆ›å»ºæ–°çš„æ–‡ä»¶ç³»ç»Ÿï¼Œå¹¶é€šè¿‡ `ceph orch` æ¥é…ç½® MDSã€‚

1. åˆ›å»º CephFS å·

    ```bash
    ceph fs volume create cephfs
    ```

    * `cephfs`ï¼šä¸º CephFS æ–‡ä»¶ç³»ç»ŸæŒ‡å®šçš„åç§°

1. åˆ©ç”¨ Ceph Orchestrator ä¸ºæ–‡ä»¶ç³»ç»Ÿè‡ªåŠ¨åˆ›å»ºå¹¶é…ç½® MDS

    ```bash
    ceph orch apply mds cephfs 'count:3'
    ```

1. éªŒè¯ cephfs çŠ¶æ€

    ```bash
    ceph fs status
    ```

    é»˜è®¤æƒ…å†µä¸‹ä¼šæœ‰ä¸€å°ä¸º activeï¼Œå…¶ä½™èŠ‚ç‚¹å‡ä¸º standbyï¼š

    ```bash
    cephfs - 0 clients
    ======
    RANK  STATE           MDS             ACTIVITY     DNS    INOS   DIRS   CAPS  
    0    active  cephfs.kube2.tekede  Reqs:    0 /s    10     13     12      0   
        POOL           TYPE     USED  AVAIL  
    cephfs.cephfs.meta  metadata  96.0k  9697M  
    cephfs.cephfs.data    data       0   9697M  
        STANDBY MDS      
    cephfs.kube1.kjracz  
    cephfs.kube3.rdjvxy  
    MDS version: ceph version 19.2.2 (0eceb0defba60152a8182f7bd87d164b639885b8) squid (stable)
    ```

## éƒ¨ç½² NFS

ä¸­å°ä¼ä¸šå†…éƒ¨é€šå¸¸é€šè¿‡æœåŠ¡å™¨æ­é… NFS æˆ– SMB åè®®å®ç°æ–‡ä»¶å…±äº«ã€‚ç”±äº Ceph å¯¹ SMB åè®®çš„æ”¯æŒå°šå¤„äºå¼€å‘é˜¶æ®µï¼ŒåŠŸèƒ½å°šä¸å®Œå–„ï¼Œå»ºè®®åœ¨ Ceph é›†ç¾¤ä¸­éƒ¨ç½² NFS Ganesha æœåŠ¡ï¼Œå°† CephFS ç›®å½•ä»¥ NFS åè®®å¯¼å‡ºä¾›å®¢æˆ·ç«¯è®¿é—®ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒCeph èƒ½ä½œä¸ºç»Ÿä¸€çš„åˆ†å¸ƒå¼å­˜å‚¨å¹³å°ï¼Œç¨³å®šé«˜æ•ˆåœ°æä¾› NFS æ–‡ä»¶å…±äº«ï¼Œæ»¡è¶³ä¼ä¸šå†…ç½‘å¤šç»ˆç«¯çš„è®¿é—®éœ€æ±‚ã€‚

ğŸ“Œæ³¨ï¼šç›®å‰ä»…æ”¯æŒ NFSv4 åè®®ã€‚

1. ç”±äºå­˜å‚¨å±‚æ˜¯ RADOSï¼Œå¤©ç„¶å…·å¤‡é«˜å¯ç”¨æ€§ï¼Œå¦‚éœ€å®ç°è®¿é—®å±‚çš„é«˜å¯ç”¨å¯å‚è€ƒå®˜æ–¹æ–‡æ¡£é…ç½® ingress å’Œ haproxy ï¼Œæœ¬æ–‡é‡‡ç”¨äº†å•èŠ‚ç‚¹æ–¹å¼è¿›è¡Œæ¼”ç¤ºï¼š

    ```bash
    ceph orch apply nfs nfs
    ```

    * `nfs`ï¼šç¬¬äºŒä¸ª nfs æŒ‡çš„æ˜¯ <svc_id>ï¼Œå¯æŒ‰éœ€å‘½å

1. éªŒè¯ NFS Ganesha çŠ¶æ€ï¼š

    ```bash
        ceph orch ls --service_type=nfs
    ```

    è¾“å‡ºå¦‚ä¸‹ï¼š

    ```bash
    NAME     PORTS  RUNNING  REFRESHED  AGE  PLACEMENT  
    nfs.nfs             1/1  3m ago     3m   count:1    
    ```

1. åœ¨ CephFS ä¸­åˆ›å»º subvolumeï¼Œä¾› NFS æµ‹è¯•ï¼š

    ```bash
    ceph fs subvolume create cephfs nfsdata
    ```

1. å¯é€šè¿‡ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹å…·ä½“çš„ pathï¼š

    ```bash
    ceph fs subvolume getpath cephfs nfsdata
    ```

    ç»“æœæ˜¾ç¤ºï¼š

    ```bash
    /volumes/_nogroup/nfsdata/8eb9327b-e923-4923-9bd9-cab893f95435/
    ```

1. åˆ©ç”¨ NFS Ganesha å¯¼å‡º CephFSï¼š

    ```bash
    ceph nfs export create cephfs --cluster-id nfs --pseudo-path /nfsdata --fsname cephfs --path /volumes/_nogroup/nfsdata/8eb9327b-e923-4923-9bd9-cab893f95435/
    ```

1. éªŒè¯æ˜¯å¦å¯¼å‡ºæˆåŠŸï¼š

    ```bash
    ceph nfs export ls nfs --detailed 
    ```

    æ£€æŸ¥æ˜¯å¦æœ‰ "cluster_id" ä¸º "nfs" çš„æ•°æ®æ¡ç›®ï¼š

    ```bash
    [
        {
            "access_type": "RW",
            "clients": [],
            "cluster_id": "nfs",
            "export_id": 1,
            "fsal": {
            "cmount_path": "/",
            "fs_name": "cephfs",
            "name": "CEPH",
            "user_id": "nfs.nfs.cephfs.405e1f9f"
            },
            "path": "/volumes/_nogroup/nfsdata/8eb9327b-e923-4923-9bd9-cab893f95435/",
            "protocols": [
                4
            ],
            "pseudo": "/nfsdata",
            "security_label": true,
            "squash": "none",
            "transports": [
            "TCP"
            ]
        }
    ]
    ```

1. å®¢æˆ·ç«¯å®‰è£… NFS è½¯ä»¶åï¼Œå®ŒæˆæŒ‚è½½ï¼š

    ```bash
    mount -t nfs4 -o nfsvers=4 192.168.0.150:/nfsdata /mnt
    ```

1. æ¥ä¸‹æ¥ä¾¿å¯è¿›å…¥ `/mnt` å®Œæˆè¯»å†™æ“ä½œï¼Œä¹Ÿå¯ä»¥åœ¨å¤šä¸ªå®¢æˆ·ç«¯æŒ‚è½½éªŒè¯ï¼Œæ¯”å¦‚æˆ‘ä»¬å¯ä»¥æµ‹è¯•å†™å…¥ä¸€ä¸ª 1GiB çš„æ–‡ä»¶ï¼š

    ```bash
    dd if=/dev/zero of=/mnt/nfsdata/8eb9327b-e923-4923-9bd9-cab893f95435/test-1GiB.bin bs=1M count=1024 status=progress
    ```

1. æ‰§è¡Œ `ceph fs status` æŸ¥çœ‹çŠ¶æ€ï¼Œå¯ä»¥çœ‹åˆ° CephFS çš„ data pool ä¸­å·²å†™å…¥ `1024M * 3` çš„æ•°æ®ï¼š

    ```bash
    cephfs - 1 clients
    ======
    RANK  STATE           MDS             ACTIVITY     DNS    INOS   DIRS   CAPS  
    0    active  cephfs.kube2.tekede  Reqs:    0 /s    23     21     16      8   
        POOL           TYPE     USED  AVAIL  
    cephfs.cephfs.meta  metadata  1119k  8646M  
    cephfs.cephfs.data    data    3072M  8646M  
        STANDBY MDS      
    cephfs.kube1.kjracz  
    cephfs.kube3.rdjvxy  
    MDS version: ceph version 19.2.2 (0eceb0defba60152a8182f7bd87d164b639885b8) squid (stable)
    ```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­å®Œæˆ [Ceph: 03-RGW éƒ¨ç½²ä¸ S3 å…¼å®¹æ¥å£æµ‹è¯•]({{< relref "posts/infra/storage/03-rgw.md" >}})ã€‚
