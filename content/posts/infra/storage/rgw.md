---
title: "åŸºäº cephadm éƒ¨ç½²é«˜å¯ç”¨ Ceph å­˜å‚¨é›†ç¾¤"
date: 2025-06-12T10:00:00+08:00
lastmod: 2025-06-12T10:00:00+08:00
description: "é€šè¿‡ cephadm å·¥å…·éƒ¨ç½²ä¼ä¸šçº§é«˜å¯ç”¨ Ceph é›†ç¾¤ï¼Œæ”¯æŒå¤šèŠ‚ç‚¹ç®¡ç†ã€è‡ªåŠ¨åŒ–è¿ç»´ã€Dashboard UIï¼Œé€‚ç”¨äºç”Ÿäº§ç¯å¢ƒã€‚"
categories: ["åŸºç¡€è®¾æ–½"]
tags: ["Ceph", "cephadm", "é«˜å¯ç”¨", "åˆ†å¸ƒå¼å­˜å‚¨", "ä¼ä¸šå®è·µ"]
series: ["éƒ¨ç½²æ–¹æ¡ˆ"]
keywords: ["cephadm é«˜å¯ç”¨", "Ceph å­˜å‚¨é›†ç¾¤", "Ceph è‡ªåŠ¨åŒ–éƒ¨ç½²"]
ShowToc: true
TocOpen: true
draft: true
---

## éƒ¨ç½²å·¥å…·é€‰å‹

Ceph æ˜¯ä¸€æ¬¾ä¼ä¸šçº§å¼€æºåˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿï¼Œå…·å¤‡é«˜å¯æ‰©å±•æ€§ä¸å¼ºä¸€è‡´æ€§ã€‚æ—©æœŸ Ceph éƒ¨ç½²ä¸»è¦é‡‡ç”¨ ceph-deploy é…åˆ Ansible çš„æ–¹å¼ï¼Œç›®å‰ï¼Œceph-deploy å·²é€æ­¥è¢«æ·˜æ±°ï¼Œè€Œ ceph-ansible é¡¹ç›®è‡ª 2022 å¹´ 8 æœˆèµ·åœæ­¢å‘å¸ƒæ–°ç‰ˆæœ¬ï¼Œç¤¾åŒºå»ºè®®è¿ç§»è‡³ cephadmã€‚ä» Octopusï¼ˆ15.2.xï¼‰ç‰ˆæœ¬èµ·ï¼ŒCeph å®˜æ–¹å¼•å…¥å¹¶æ¨èä½¿ç”¨åŸºäºå®¹å™¨çš„éƒ¨ç½²ä¸è¿ç»´å·¥å…· cephadmï¼Œç”¨äºç»Ÿä¸€ç®¡ç†é›†ç¾¤çš„éƒ¨ç½²ã€å‡çº§ä¸æœåŠ¡ç¼–æ’ã€‚

åœ¨ Kubernetes ç¯å¢ƒä¸­ï¼ŒRook æä¾›åŸç”Ÿæ–¹å¼æ¥æ¶ˆè´¹ Ceph å­˜å‚¨ï¼Œæ˜¯å½“å‰æœ€ä¸»æµçš„é›†æˆæ–¹æ¡ˆã€‚Rook æ”¯æŒéƒ¨ç½²ç‹¬ç«‹ Ceph é›†ç¾¤ï¼Œä¹Ÿæ”¯æŒæ¥å…¥å¤–éƒ¨å·²éƒ¨ç½²çš„ Ceph é›†ç¾¤ï¼ˆExternal Cluster æ¨¡å¼ï¼‰ã€‚

ç„¶è€Œï¼ŒRook å­˜åœ¨ä»¥ä¸‹å±€é™æ€§ï¼š

* Ceph åœ¨é«˜è´Ÿè½½æˆ–æ¢å¤åœºæ™¯ä¸‹å¯¹åº•å±‚ç¡¬ä»¶æ€§èƒ½å’Œç½‘ç»œå»¶è¿Ÿæ•æ„Ÿï¼ŒRook ä¾èµ–äº Kubernetes é›†ç¾¤æœ¬èº«çš„ç¨³å®šæ€§ï¼Œæ•…éšœæ’æŸ¥é“¾è·¯è¾ƒé•¿ï¼Œæç«¯åœºæ™¯ä¸‹å¯èƒ½éœ€è¦é¢å¤–è€ƒè™‘ K8s è°ƒåº¦ã€ç½‘ç»œã€Pod ç”Ÿå‘½å‘¨æœŸç­‰å¤šä¸ªç»´åº¦
* Rook è®¾è®¡ä¸Šæ›´é€‚åˆä¸€ä¸ª K8s é›†ç¾¤éƒ¨ç½²ä¸€å¥—ç‹¬ç«‹çš„ Ceph é›†ç¾¤ï¼Œè™½ç„¶è§£è€¦æ€§å¥½ï¼Œä½†æ˜¯å­˜å‚¨èµ„æºåœ¨é›†ç¾¤é—´éš¾ä»¥æœç”¨ï¼Œèµ„æºæˆæœ¬å’Œè¿ç»´å¤æ‚åº¦éƒ½ä¸å°
* Rook åªé€‚åˆäº‘åŸç”Ÿåœºæ™¯ï¼Œä¸å…¼å®¹è£¸é‡‘å±ã€ç§æœ‰äº‘ã€å…¬æœ‰äº‘ç­‰æ··åˆæ¶æ„ä¸‹ï¼Œéš¾ä»¥ä½œä¸ºç»Ÿä¸€ç®¡ç†çš„åº•å±‚å­˜å‚¨å¹³å°

æœ¬æ–‡é‡‡ç”¨ cephadm éƒ¨ç½² Ceph é›†ç¾¤ï¼ŒRook ä½œä¸ºæ¶ˆè´¹ç«¯æ¥å…¥çš„æ¶æ„ï¼Œå…¼é¡¾äº†è¿ç»´å¯æ§æ€§ä¸äº‘åŸç”Ÿç”Ÿæ€çš„é›†æˆèƒ½åŠ›ï¼Œé€‚ç”¨äºæ··åˆæ¶æ„ä¸‹çš„ç»Ÿä¸€å­˜å‚¨åœºæ™¯ã€‚å¯¹äº Dev/Test/Staging ç­‰éç”Ÿäº§ç¯å¢ƒï¼Œå¯å…±ç”¨ä¸€å¥—ç”± cephadm éƒ¨ç½²çš„ä¸­å°å‹é›†ç¾¤ï¼›å¯¹äº Prod ç¯å¢ƒï¼Œå»ºè®®ç‹¬ç«‹éƒ¨ç½²ä¸€å¥— Ceph é›†ç¾¤ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™ç§æ–¹å¼å¯ä»¥å¾ˆå¥½çš„é€‚é…æ··åˆæ¶æ„ï¼Œé¿å… Kubernetes æœ¬èº«çš„ä¸ç¨³å®šå½±å“ï¼Œå¹¶é™ä½é‡å¤éƒ¨ç½²æˆæœ¬ã€‚

## Ceph ä½œä¸ºç»Ÿä¸€å­˜å‚¨å¹³å°

Ceph æ”¯æŒå—ï¼ˆRBDï¼‰ã€æ–‡ä»¶ï¼ˆCephFSï¼‰ã€å¯¹è±¡ï¼ˆRGWï¼‰ç­‰å¤šç§å­˜å‚¨åè®®ï¼š

| åè®®ç±»å‹        | åœºæ™¯                                |
| --------------- | ----------------------------------- |
| RBD å—å­˜å‚¨      | MySQLã€PostgreSQLã€MongoDB ç­‰æ•°æ®åº“ |
| CephFS æ–‡ä»¶å­˜å‚¨ | å…±äº«æŒ‚è½½ã€æ—¥å¿—å­˜å‚¨ã€NFS æœåŠ¡        |
| RGW å¯¹è±¡å­˜å‚¨    | S3ã€å¤‡ä»½ã€AI æ¨¡å‹ç®¡ç†               |

Ceph ä½œä¸ºç»Ÿä¸€åº•å±‚å­˜å‚¨å¹³å°æ˜¯ä¸€ä¸ªä¼˜ç§€æ–¹æ¡ˆï¼Œä½†å¯¹äºå¯¹ I/O å»¶è¿Ÿæå…¶æ•æ„Ÿçš„å­˜å‚¨åœºæ™¯ï¼ˆå¦‚ etcdã€Redis ç­‰ï¼‰ï¼Œå…¶è¡¨ç°å¯èƒ½ä¸è¶³ï¼Œæ­¤æ—¶å»ºè®®è€ƒè™‘ä»¥ä¸‹å­˜å‚¨ç­–ç•¥ï¼š

* Local PV + StatefulSetï¼šé€šè¿‡é…ç½®èŠ‚ç‚¹äº²å’Œæ€§å¹¶ä½¿ç”¨èŠ‚ç‚¹æœ¬åœ°ç¡¬ç›˜ï¼ˆSSD/HDDï¼‰ï¼Œé€‚ç”¨äºå¯¹æ€§èƒ½æ•æ„Ÿçš„æœåŠ¡ï¼Œæä¾›é«˜ IOPS å’Œä½å»¶è¿Ÿ
* å¤–éƒ¨éƒ¨ç½²æˆ–äº‘å­˜å‚¨æœåŠ¡ï¼šè¿½æ±‚ K8s é›†ç¾¤æ— çŠ¶æ€ï¼Œå°†å­˜å‚¨æœåŠ¡ä¸ K8s å®Œå…¨è§£è€¦ï¼ŒKubernetes é›†ç¾¤å˜åŠ¨ã€è¿ç§»ã€å‡çº§æ—¶ï¼Œå­˜å‚¨æœåŠ¡å¯å®ç°æ— æ„ŸçŸ¥ï¼Œæé«˜å¯ç»´æŠ¤æ€§å’Œå¼¹æ€§

## éƒ¨ç½²é«˜å¯ç”¨çš„ ceph é›†ç¾¤

ä¸‹é¢æˆ‘ä»¬å°†ä½¿ç”¨ cephadm åœ¨æ¥æ„å»ºä¸€ä¸ªä¸‰èŠ‚ç‚¹çš„é«˜å¯ç”¨ Ceph é›†ç¾¤ã€‚

æ ¸å¿ƒç»„ä»¶ç‰ˆæœ¬ï¼š

```yaml
OS: Debian GNU/Linux 12ï¼ˆbookwormï¼‰
Podman: 4.3.1
Ceph: 19.2.2ï¼ˆsquidï¼‰
Kubernetes: 1.31.7
Rook: v1.17.0
```

ğŸ“Œæ³¨ï¼šå¼ºçƒˆæ¨èä½¿ç”¨ Podman ä»£æ›¿ Dockerï¼ŒPodman æ”¯æŒ rootless æ¨¡å¼ã€æ— å®ˆæŠ¤è¿›ç¨‹ï¼ˆdaemonlessï¼‰è¿è¡Œï¼Œä¸”å¯åŸç”Ÿé›†æˆ systemd å•å…ƒæœåŠ¡ï¼Œç³»ç»Ÿäº²å’Œæ€§æ›´å¼ºã€‚ç”±äº Red Hat åŒæ—¶æ˜¯ Ceph ä¸ Podman çš„ä¸»è¦ç»´æŠ¤æ–¹ï¼Œä½¿ç”¨ Podman èƒ½è·å¾—æ›´å¥½çš„å…¼å®¹æ€§ã€å®‰å…¨æ€§ä¸å®˜æ–¹æ”¯æŒä¿éšœã€‚

éƒ¨ç½²å‰ç¡®è®¤ä»¥ä¸‹äº‹é¡¹ï¼š

* ä¸‰å°è£¸æœºæˆ–è™šæ‹Ÿæœºï¼ˆè¿™é‡Œä»¥ 192.168.0.{150..152} ä¸‰å°ä¸ºä¾‹ï¼‰
* æ¯ä¸ªèŠ‚ç‚¹è‡³å°‘ä¸º Ceph å‡†å¤‡ä¸€å—æ•°æ®ç›˜ï¼ˆæ— éœ€æ ¼å¼åŒ–ï¼‰
* æ“ä½œç³»ç»Ÿæ”¯æŒ Python3ã€Systemdã€Podman/Dockerã€LVM2ï¼ˆç›®å‰ä¸»æµæ“ä½œç³»ç»ŸåŸºæœ¬éƒ½æ”¯æŒï¼‰
* å·²è¿›è¡Œæ—¶é—´åŒæ­¥

å…·ä½“éƒ¨ç½²æµç¨‹ï¼š

1. ssh åˆ°ä¸»æ§èŠ‚ç‚¹ï¼Œç›´æ¥å®‰è£… cephadmï¼ˆä¹Ÿå¯ä»¥ä½¿ç”¨ curl æ–¹å¼ï¼‰ï¼Œä¸ç”¨æ‹…å¿ƒå‘è¡Œç‰ˆä¸­ cephadm ç‰ˆæœ¬è¿‡æ—§çš„é—®é¢˜ï¼Œè¿™é‡Œåªæ˜¯ä¸ºäº†é€šè¿‡è¯¥å‘½ä»¤é…ç½®ä»“åº“ã€‚

    ```bash
    apt install -y cephadm
    ```

1. é€šè¿‡ cephadm æ·»åŠ  Ceph å®˜æ–¹æœ€æ–°ç¨³å®šç‰ˆä»“åº“ï¼Œå¹¶å®‰è£…æœ€æ–°çš„ cephadm ä¾èµ–åŒ…

    ```bash
    cephadm add-repo --release squid
    cephadm install
    ```

1. æ£€æŸ¥ PATH å˜é‡

    ```bash
    which cephadm
    ```

    æˆåŠŸå°†è¿”å›ä»¥ä¸‹ä¿¡æ¯ï¼š

    ```bash
    /usr/sbin/cephadm
    ```

1. å¼•å¯¼åˆ›å»ºæ–°é›†ç¾¤

    ```bash
    cephadm bootstrap --mon-ip 192.168.0.150
    ```

    è¯¥æ¡å‘½ä»¤åšäº†ä»¥ä¸‹å‡ ä»¶äº‹ï¼š

    * åœ¨æœ¬åœ°ä¸»æœºä¸Šåˆ›å»º MON å’Œ MGR å®ˆæŠ¤è¿›ç¨‹
    * ç”Ÿæˆé›†ç¾¤å¯†é’¥
    * é…ç½®äº†æœ€å°åŒ–çš„ ceph.conf æ–‡ä»¶åŠ admin å¯†é’¥æ–‡ä»¶
    * å°†å½“å‰ä¸»æœºæ‰“ä¸Š _admin æ ‡ç­¾ä»¥ä¾¿ç®¡ç†è®¿é—®

    è¿™é‡Œé¢å¤–ç»™å‡ºå‡ ä¸ªå¸¸ç”¨çš„å‚æ•°ï¼Œå¯æ‰§è¡Œ `cephadm bootstrap -h` æŸ¥çœ‹æ›´å¤šå‚æ•°ï¼š
    * `--allow-fqdn-hostname`ï¼šå…è®¸ä½¿ç”¨ FQDN ä½œä¸ºä¸»æœºå
    * `--log-to-file`ï¼šè¾“å‡ºæ—¥å¿—åˆ°æœ¬åœ°æ–‡ä»¶
    * `--public-network`ï¼šæŒ‡å®šå®¢æˆ·ç«¯ä¸ MONã€OSDã€MDSã€RGW ç­‰ Ceph ç»„ä»¶é€šä¿¡çš„ç½‘ç»œ
    * `--cluster-network`ï¼šæŒ‡å®š OSD èŠ‚ç‚¹é—´ä¸“ç”¨çš„å†…éƒ¨ç½‘ç»œï¼Œç”¨äºæ•°æ®å¤åˆ¶ã€æ¢å¤å’Œå¿ƒè·³æ£€æµ‹
    * `--config`ï¼šæŒ‡å®šè‡ªå®šä¹‰çš„é…ç½®æ–‡ä»¶è·¯å¾„

1. å¯ç”¨ ceph cli

    è¦æ‰§è¡Œ ceph å‘½ä»¤ï¼Œéœ€è¦é€šè¿‡ ceph shell çš„æ–¹å¼åˆ°å®¹å™¨ä¸­è¿è¡Œï¼š

    ```bash
    cephadm shell -- ceph -s
    ```

    ä¸ºäº†æ–¹ä¾¿è¿ç»´ï¼Œå»ºè®®å®‰è£… ceph-common è½¯ä»¶åŒ…ï¼Œé‡Œé¢åŒ…å«äº†æ‰€æœ‰ ceph å‘½ä»¤ï¼ŒåŒ…æ‹¬ cephã€rbdã€mount.cephï¼ˆç”¨äºæŒ‚è½½CephFSæ–‡ä»¶ç³»ç»Ÿï¼‰ç­‰ï¼š

    ```bash
    cephadm install ceph-common
    ```

    æ¥ä¸‹æ¥ï¼Œä¾¿å¯ä»¥ç›´æ¥ä½¿ç”¨ ceph å‘½ä»¤äº†ï¼Œæ¯”å¦‚æŸ¥çœ‹ ceph é›†ç¾¤çŠ¶æ€ï¼š

    ```bash
    ceph status
    ```

    åº”è¯¥ä¼šçœ‹åˆ°ä¸€ä¸ª mon å’Œ ä¸€ä¸ª mgr è¿›ç¨‹ï¼š

    ```bash
    cluster:
        id:     852837fa-46a1-11f0-a6ed-00505639e7ae
        health: HEALTH_WARN
                OSD count 0 < osd_pool_default_size 3
    
    services:
        mon: 1 daemons, quorum kube1 (age 2m)
        mgr: kube1.iuiljm(active, since 54s)
        osd: 0 osds: 0 up, 0 in
    
    data:
        pools:   0 pools, 0 pgs
        objects: 0 objects, 0 B
        usage:   0 B used, 0 B / 0 B avail
        pgs:   
    ```

1. æ·»åŠ å…¶ä»–ä¸»æœºåˆ°é›†ç¾¤ä¸­ï¼š

    å°†é›†ç¾¤çš„å…¬å…± SSH å¯†é’¥å®‰è£…åˆ°æ–°ä¸»æœº root ç”¨æˆ·çš„ authorized_keys æ–‡ä»¶ä¸­ï¼š
  
    ```bash
    ssh-copy-id -f -i /etc/ceph/ceph.pub root@kube2
    ssh-copy-id -f -i /etc/ceph/ceph.pub root@kube3
    ```

    é€šçŸ¥ Ceph æœ‰æ–°èŠ‚ç‚¹åŠ å…¥é›†ç¾¤ï¼š

    ```bash
    ceph orch host add kube2 192.168.0.151 --labels _admin
    ceph orch host add kube3 192.168.0.152 --labels _admin
    ```

    ğŸ“Œæ³¨ï¼šé»˜è®¤æƒ…å†µä¸‹ï¼Œ_admin æ ‡ç­¾ä¼šè®© cephadm åœ¨è¯¥ä¸»æœºçš„ /etc/ceph ç›®å½•ä¸‹ç»´æŠ¤ä¸€ä»½ ceph.conf é…ç½®æ–‡ä»¶å’Œ client.admin å¯†é’¥ç¯æ–‡ä»¶ï¼Œé€‚ç”¨äºéƒ¨ç½² MONã€MGR ç­‰å…³é”®æœåŠ¡èŠ‚ç‚¹ã€‚

    å¯ä»¥é€šè¿‡æ­¤å‘½ä»¤æŸ¥çœ‹å„èŠ‚ç‚¹æœåŠ¡åˆ—è¡¨ï¼š

    ```bash
    ceph orch ls
    ```

    å¦‚æœå‘ç°è¾“å‡ºç»“æœæœ‰ç‚¹ä¸ç¬¦é¢„æœŸï¼Œé‚£æ˜¯å› ä¸º `cephadm boostrap` é»˜è®¤ç­–ç•¥ä¸‹ mon æ•°é‡ä¸º 5ï¼Œç”±äºæˆ‘è¿™é‡Œåªæœ‰ 3 ä¸ªèŠ‚ç‚¹ï¼Œæ‰€ä»¥éœ€è¦æ‰‹åŠ¨å°† mon æ•°é‡è°ƒæ•´æˆ 3ï¼š

    ```bash
    ceph orch apply mon count:3
    ```

    è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

    ```bash
    NAME           PORTS        RUNNING  REFRESHED  AGE  PLACEMENT  
    alertmanager   ?:9093,9094      1/1  30s ago    5m   count:1    
    ceph-exporter                   3/3  30s ago    5m   *          
    crash                           3/3  30s ago    6m   *          
    grafana        ?:3000           1/1  30s ago    5m   count:1    
    mgr                             2/2  30s ago    6m   count:2    
    mon                             3/3  30s ago    1s   count:3    
    node-exporter  ?:9100           3/3  30s ago    5m   *          
    prometheus     ?:9095           1/1  30s ago    5m   count:1    
    ```

1. æ·»åŠ  OSD å­˜å‚¨ï¼š

    1. æ£€æŸ¥æ‰€æœ‰é›†ç¾¤ä¸»æœºä¸Šçš„å­˜å‚¨è®¾å¤‡ä¿¡æ¯ï¼š

        ```bash
        ceph orch device ls
        ```

        å­˜å‚¨è®¾å¤‡å¯ç”¨çš„å‰ææ˜¯ï¼š
        * è®¾å¤‡ä¸Šä¸èƒ½æœ‰ä»»ä½•åˆ†åŒº
        * è®¾å¤‡ä¸Šä¸èƒ½å­˜åœ¨ä»»ä½• LVM çŠ¶æ€
        * è®¾å¤‡ä¸èƒ½è¢«æŒ‚è½½
        * è®¾å¤‡ä¸Šä¸èƒ½åŒ…å«æ–‡ä»¶ç³»ç»Ÿ
        * è®¾å¤‡ä¸Šä¸èƒ½åŒ…å« Ceph BlueStore OSD
        * è®¾å¤‡å®¹é‡å¿…é¡»å¤§äº 5 GB
        * Ceph ä¸ä¼šåœ¨ä¸ç¬¦åˆæ¡ä»¶çš„è®¾å¤‡ä¸Šåˆ›å»º OSD

    1. æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ä»¥é¢„æ¼”é…ç½®å˜æ›´ï¼ˆå¯èƒ½éœ€è¦æ‰§è¡Œä¸¤æ¬¡ä¹Ÿä¼šæ˜¾ç¤ºç»“æœï¼‰ï¼š

        ```bash
        ceph orch apply osd --all-available-devices --dry-run
        ```

        ğŸ“Œæ³¨ï¼š`ceph orch apply` ä¼šè®© cephadm æŒç»­å¯¹çŠ¶æ€è¿›è¡Œåè°ƒï¼ˆreconcileï¼‰ï¼Œä¿è¯é›†ç¾¤çŠ¶æ€ä¸æœŸæœ›é…ç½®ä¸€è‡´ã€‚
        æ¢å¥è¯è¯´ï¼ŒCeph é›†ç¾¤ä¼šæŒç»­ç›‘æ§å’Œè‡ªåŠ¨ç®¡ç†ç¬¦åˆæ¡ä»¶çš„è®¾å¤‡ï¼š
          * æ–°åŠ å…¥é›†ç¾¤çš„ç£ç›˜ä¼šè¢«è‡ªåŠ¨å‘ç°å¹¶ç”¨äºåˆ›å»ºæ–°çš„ OSD
          * å¦‚æœæŸä¸ª OSD è¢«ç§»é™¤ä¸”å¯¹åº”çš„ LVM ç‰©ç†å·è¢«æ¸…ç†ï¼ˆzapï¼‰ï¼ŒCeph ä¹Ÿä¼šè‡ªåŠ¨åœ¨è¯¥è®¾å¤‡ä¸Šé‡æ–°åˆ›å»ºæ–°çš„ OSD

    1. ç»§ç»­æ‰§è¡Œå®Œæˆ osd åˆ›å»ºï¼Œæ•æ„Ÿç¯å¢ƒä¸­å¯ä»¥è¿½åŠ  `--unmanaged` å‚æ•°æ”¹ç”¨æ‰‹åŠ¨ç®¡ç†ä»¥é¿å…è¯¯æ“ä½œï¼š

        ```bash
        ceph orch apply osd --all-available-devices
        ```

    1. æˆ–è€…ä½ ä¹Ÿå¯ä»¥æ‰‹åŠ¨æŒ‡å®šï¼Œè¿™æ ·æ›´å®‰å…¨å¯æ§ï¼ˆå¯é€‰ï¼‰

        ```bash
        ceph orch daemon add osd *<host>*:*<device-path>*
        ```

    1. ç¨å¾®ä¸€ä¼šï¼Œæ£€æŸ¥é›†ç¾¤çŠ¶æ€ï¼Œæ˜¾ç¤ºå·²æœ‰ 3 ä¸ª osd åŠ å…¥åˆ°é›†ç¾¤ä¸­ï¼Œä¸”ä¸º up çŠ¶æ€ï¼š

        ```bash
        cluster:
            id:     852837fa-46a1-11f0-a6ed-00505639e7ae
            health: HEALTH_OK
        
        services:
            mon: 3 daemons, quorum kube1,kube2,kube3 (age 22m)
            mgr: kube1.iuiljm(active, since 24m), standbys: kube2.mpdvnm
            osd: 3 osds: 3 up (since 68s), 3 in (since 2m)
        
        data:
            pools:   1 pools, 1 pgs
            objects: 2 objects, 449 KiB
            usage:   81 MiB used, 30 GiB / 30 GiB avail
            pgs:     1 active+clean  
        ```

1. å¼€å¯å†…å­˜è‡ªåŠ¨è°ƒèŠ‚é€‰é¡¹

    cephadm å¼•å¯¼é›†ç¾¤æ—¶é»˜è®¤ä¼šå¯ç”¨ `osd_memory_target_autotune = true`ï¼Œå¹¶è®¾ç½® `mgr/cephadm/autotune_memory_target_ratio = 0.7`ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯å°ä¸»æœºçš„ OSD é»˜è®¤æœ€å¤šä½¿ç”¨è¯¥èŠ‚ç‚¹æ€»å†…å­˜çš„ 70%ï¼Œç”¨äº BlueStore çš„ç¼“å­˜ï¼Œå½“ä½ çš„å­˜å‚¨é›†ç¾¤ä¸Šæœ‰å…¶ä»–æœåŠ¡æ—¶ï¼Œå¯å»ºè®®æŒ‰éœ€è°ƒå°è¯¥å‚æ•°ï¼Œé˜²æ­¢ OOMã€‚

    ```bash
    ceph config set osd osd_memory_target_autotune true
    ceph config set mgr mgr/cephadm/autotune_memory_target_ratio 0.3
    ```

1. å¼€å¯ dashboard

    1. é€šè¿‡ä»¥ä¸‹å‘½ä»¤å¯ç”¨ dashboardï¼š

    ```bash
    ceph mgr module enable dashboard
    ```

    1. è®¾ç½® SSL/TLS æ”¯æŒï¼Œè¿™é‡Œä»…ä½¿ç”¨è‡ªç­¾åè¯ä¹¦ï¼š

    ```bash
    ceph dashboard create-self-signed-cert
    ```

    1. åˆ›å»ºä¸€ä¸ªç®¡ç†å‘˜ç”¨æˆ· adminï¼Œè¿™é‡Œä½¿ç”¨ `--force-password` ç»•è¿‡å¯†ç ç­–ç•¥æ£€æŸ¥ï¼Œå¹¶ä½¿ç”¨ `--pwd_update_required` ç¡®ä¿é¦–æ¬¡ç™»å½•åå¼ºåˆ¶è¦æ±‚ç”¨æˆ·æ›´æ”¹å¯†ç ï¼š

    ```bash
    echo "123456" > admin.pass
    ceph dashboard ac-user-create --force-password --pwd_update_required admin -i admin.pass administrator
    ```

    1. ç›®å‰æˆ‘ä»¬çš„é›†ç¾¤é‡Œæœ‰ä¸¤ä¸ª mgrï¼Œä¸ºäº†æé«˜å¯ç”¨æ€§ï¼Œæˆ‘ä»¬éœ€è¦å¼•å…¥ä»£ç†æœåŠ¡ï¼Œå»ºè®®å…ˆæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

    ```bash
    ceph config set mgr mgr/dashboard/standby_behaviour "error"
    ceph config set mgr mgr/dashboard/standby_error_status_code 503
    ```

    ğŸ“Œæ³¨ï¼šä¸Šè¿°å‘½ä»¤ä¼šè®©é active çš„ mgr dashboard ä¸å“åº” 303 Redirectï¼Œè€Œæ˜¯è‡ªå®šä¹‰ç›¸åº” 503 çŠ¶æ€ç ï¼Œå¦åˆ™ä»£ç†æœåŠ¡åœ¨ dashboard failover åå°†æ”¶åˆ°é”™è¯¯è·³è½¬åœ°å€ã€‚

    1. å®˜æ–¹ç»™çš„æ˜¯ haproxy ç¤ºä¾‹ï¼Œæˆ‘çš„ç¯å¢ƒä»…éƒ¨ç½²äº† openrestyï¼Œå› æ­¤è¿™é‡Œç»™å‡º openresty çš„åŸºæœ¬é…ç½®ï¼Œå¦‚æœä¸¤ä¸ªèŠ‚ç‚¹ä»¥ä¸Šå»ºè®®å¼•å…¥ health check å®ç°åŠ¨æ€ upstreamï¼š

    ```bash
    upstream ceph-dashboard-backend {
        server 192.168.0.150:8443;
        server 192.168.0.151:8443;
    }

    server {
        listen 443 ssl;
        server_name ceph-dashboard.example.internal;

        include /etc/openresty/conf.d/ssl.conf;

        access_log /var/log/openresty/ceph-dashboard-access.log;
        error_log  /var/log/openresty/ceph-dashboard-error.log;

        location / {
            proxy_pass https://ceph-dashboard-backend;

            proxy_ssl_server_name on;
            proxy_ssl_verify off;

            proxy_connect_timeout 5s;
            proxy_send_timeout 30s;
            proxy_read_timeout 30s;

            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;

            proxy_next_upstream error timeout http_503;
        }
    }
    ```

1. æ·»åŠ  CephFS

    ![CephFS æ¶æ„å›¾](/images/posts/cephfs-architecture.svg)

    Ceph æ–‡ä»¶ç³»ç»Ÿ ( CephFS ) æ˜¯ä¸€ä¸ªç¬¦åˆ POSIX æ ‡å‡†çš„æ–‡ä»¶ç³»ç»Ÿï¼Œå®ƒæ„å»ºäº Ceph çš„åˆ†å¸ƒå¼å¯¹è±¡å­˜å‚¨RADOSä¹‹ä¸Šã€‚

    è¦ä½¿ç”¨ CephFS æ–‡ä»¶ç³»ç»Ÿï¼Œéœ€è¦ä¸€ä¸ªæˆ–å¤šä¸ª MDSï¼ˆMetadata Serverï¼‰å®ˆæŠ¤è¿›ç¨‹ã€‚å¦‚æœä½¿ç”¨è¾ƒæ–°çš„ `ceph fs volume` æ¥å£æ¥åˆ›å»ºæ–°çš„æ–‡ä»¶ç³»ç»Ÿï¼Œè¿™äº›å®ˆæŠ¤è¿›ç¨‹ä¼šè‡ªåŠ¨åˆ›å»ºã€‚

    1. åˆ›å»º CephFS å·

        ```bash
        ceph fs volume create cephfs
        ```

        * `cephfs`ï¼šä¸º CephFS æ–‡ä»¶ç³»ç»ŸæŒ‡å®šçš„åç§°

    1. åˆ©ç”¨ Ceph Orchestrator ä¸ºæ–‡ä»¶ç³»ç»Ÿè‡ªåŠ¨åˆ›å»ºå¹¶é…ç½® MDS

        ```bash
        ceph orch apply mds cephfs 'count:3'
        ```

    1. éªŒè¯ cephfs çŠ¶æ€

        ```bash
        ceph fs status
        ```

        é»˜è®¤æƒ…å†µä¸‹ä¼šæœ‰ä¸€å°ä¸º activeï¼Œå…¶ä½™èŠ‚ç‚¹å‡ä¸º standbyï¼š

        ```bash
        cephfs - 0 clients
        ======
        RANK  STATE           MDS             ACTIVITY     DNS    INOS   DIRS   CAPS  
        0    active  cephfs.kube2.tekede  Reqs:    0 /s    10     13     12      0   
            POOL           TYPE     USED  AVAIL  
        cephfs.cephfs.meta  metadata  96.0k  9697M  
        cephfs.cephfs.data    data       0   9697M  
            STANDBY MDS      
        cephfs.kube1.kjracz  
        cephfs.kube3.rdjvxy  
        MDS version: ceph version 19.2.2 (0eceb0defba60152a8182f7bd87d164b639885b8) squid (stable)
        ```

1. æ·»åŠ  NFSï¼ˆå¯é€‰ï¼‰

    ä¸­å°ä¼ä¸šå†…éƒ¨é€šå¸¸é€šè¿‡æœåŠ¡å™¨æ­é… NFS æˆ– SMB åè®®å®ç°æ–‡ä»¶å…±äº«ã€‚ç”±äº Ceph å¯¹ SMB åè®®çš„æ”¯æŒå°šå¤„äºå¼€å‘é˜¶æ®µï¼ŒåŠŸèƒ½å°šä¸å®Œå–„ï¼Œå»ºè®®åœ¨ Ceph é›†ç¾¤ä¸­éƒ¨ç½² NFS Ganesha æœåŠ¡ï¼Œå°† CephFS ç›®å½•ä»¥ NFS åè®®å¯¼å‡ºä¾›å®¢æˆ·ç«¯è®¿é—®ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒCeph èƒ½ä½œä¸ºç»Ÿä¸€çš„åˆ†å¸ƒå¼å­˜å‚¨å¹³å°ï¼Œç¨³å®šé«˜æ•ˆåœ°æä¾› NFS æ–‡ä»¶å…±äº«ï¼Œæ»¡è¶³ä¼ä¸šå†…ç½‘å¤šç»ˆç«¯çš„è®¿é—®éœ€æ±‚ã€‚

    ğŸ“Œæ³¨ï¼šç›®å‰ä»…æ”¯æŒ NFSv4 åè®®ã€‚

    1. ç”±äºå­˜å‚¨å±‚æ˜¯ RADOSï¼Œå¤©ç„¶å…·å¤‡é«˜å¯ç”¨æ€§ï¼Œå¦‚æœéœ€è¦å¯ä»¥å‚è€ƒå®˜æ–¹æ–‡æ¡£é…ç½® ingress å’Œ haproxy å®ç°è®¿é—®å±‚çš„é«˜å¯ç”¨ï¼Œæœ¬æ–‡é‡‡ç”¨äº†å•èŠ‚ç‚¹æ–¹å¼è¿›è¡Œæ¼”ç¤ºï¼š

        ```bash
        ceph orch apply nfs nfs
        ```

    1. éªŒè¯ NFS Ganesha çŠ¶æ€ï¼š

        ```bash
            ceph orch ls --service_type=nfs
        ```

        è¾“å‡ºå¦‚ä¸‹ï¼š

        ```bash
        NAME     PORTS  RUNNING  REFRESHED  AGE  PLACEMENT  
        nfs.nfs             1/1  3m ago     3m   count:1    
        ```

    1. åœ¨ CephFS ä¸­åˆ›å»º subvolumeï¼Œä¸“ä¾› NFS æµ‹è¯•ï¼š

        ```bash
        ceph fs subvolume create cephfs nfsdata
        ```

    1. å¯é€šè¿‡ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹å…·ä½“çš„ pathï¼š

        ```bash
        ceph fs subvolume getpath cephfs nfsdata
        ```

    1. åˆ©ç”¨ NFS Ganesha å¯¼å‡º CephFSï¼š

        ```bash
        ceph nfs export create cephfs --cluster-id nfs --pseudo-path /nfsdata --fsname cephfs --path /volumes/_nogroup/nfsdata/8eb9327b-e923-4923-9bd9-cab893f95435/
        ```

    1. éªŒè¯æ˜¯å¦å¯¼å‡ºæˆåŠŸï¼š

        ```bash
        ceph nfs export ls nfs --detailed 
        ```

        æ£€æŸ¥æ˜¯å¦æœ‰ "cluster_id" ä¸º "nfs" çš„æ•°æ®æ¡ç›®ï¼š

        ```bash
        [
        {
            "access_type": "RW",
            "clients": [],
            "cluster_id": "nfs",
            "export_id": 1,
            "fsal": {
            "cmount_path": "/",
            "fs_name": "cephfs",
            "name": "CEPH",
            "user_id": "nfs.nfs.cephfs.405e1f9f"
            },
            "path": "/volumes/_nogroup/nfsdata/8eb9327b-e923-4923-9bd9-cab893f95435/",
            "protocols": [
            4
            ],
            "pseudo": "/nfsdata",
            "security_label": true,
            "squash": "none",
            "transports": [
            "TCP"
            ]
        }
        ]
        ```

    1. å®¢æˆ·ç«¯å®‰è£… NFS è½¯ä»¶åï¼Œå®ŒæˆæŒ‚è½½ï¼š

        ```bash
        mount -t nfs4 -o nfsvers=4 192.168.0.150:/nfsdata /mnt
        ```

    1. æ¥ä¸‹æ¥ä¾¿å¯è¿›å…¥ `/mnt` å®Œæˆè¯»å†™æ“ä½œï¼Œä¹Ÿå¯ä»¥åœ¨å¤šä¸ªå®¢æˆ·ç«¯æŒ‚è½½éªŒè¯ï¼Œæ¯”å¦‚æˆ‘ä»¬å¯ä»¥æµ‹è¯•å†™å…¥ä¸€ä¸ª 1GiB çš„æ–‡ä»¶ï¼š

        ```bash
        dd if=/dev/zero of=/mnt/nfsdata/8eb9327b-e923-4923-9bd9-cab893f95435/test-1GiB.bin bs=1M count=1024 status=progress
        ```

    1. æ‰§è¡Œ `ceph fs status` æŸ¥çœ‹çŠ¶æ€ï¼Œå¯ä»¥çœ‹åˆ° CephFS çš„ data pool ä¸­å·²å†™å…¥ `1024M * 3` çš„æ•°æ®ï¼š

        ```bash
        cephfs - 1 clients
        ======
        RANK  STATE           MDS             ACTIVITY     DNS    INOS   DIRS   CAPS  
        0    active  cephfs.kube2.tekede  Reqs:    0 /s    23     21     16      8   
            POOL           TYPE     USED  AVAIL  
        cephfs.cephfs.meta  metadata  1119k  8646M  
        cephfs.cephfs.data    data    3072M  8646M  
            STANDBY MDS      
        cephfs.kube1.kjracz  
        cephfs.kube3.rdjvxy  
        MDS version: ceph version 19.2.2 (0eceb0defba60152a8182f7bd87d164b639885b8) squid (stable)
        ```

1. æ·»åŠ  RGW

    RGWï¼ˆRADOS Gatewayï¼‰æ˜¯ Ceph æä¾›çš„å¯¹è±¡å­˜å‚¨ç½‘å…³æœåŠ¡ï¼ŒåŸºäº HTTP åè®®ï¼Œå®ç°äº†å¯¹ Ceph RADOS åˆ†å¸ƒå¼å¯¹è±¡å­˜å‚¨çš„è®¿é—®æ¥å£ï¼Œå…¼å®¹äº† Amazon S3 å’Œ OpenStack Swift æ¥å£ï¼Œå¯éƒ¨ç½²åœ¨å•é›†ç¾¤æˆ–å¤šç«™ç‚¹æ¨¡å¼ä¸‹ï¼Œé€‚ç”¨äºç§æœ‰äº‘ä¸æ··åˆäº‘ç¯å¢ƒã€‚
    ä¸­å°ä¼ä¸šé€šå¸¸å• Realm + å• Zone å³å¯æ»¡è¶³éœ€æ±‚ï¼Œæ­¤å¤„éƒ¨ç½²äº† 3 ä¸ª RGW å®ä¾‹ï¼Œæä¾› S3 å…¼å®¹çš„å¯¹è±¡å­˜å‚¨æœåŠ¡ï¼Œå¹¶é¢„ç•™äº†åŸºäºå¤šç«™ç‚¹ï¼ˆMultisiteï¼‰æ¶æ„çš„æ‰©å±•èƒ½åŠ›ã€‚ã€‚

    1. ä½¿ç”¨ Multisite æ¶æ„åˆå§‹åŒ–ï¼š

        ```bash
        # åˆå§‹åŒ–ä¸€ä¸ª Realm
        radosgw-admin realm create --rgw-realm=default --default

        # åˆ›å»ºä¸€ä¸ª ZoneGroupï¼Œè®¾ä¸ºé»˜è®¤ä¸”ä¸ºä¸» ZoneGroup
        radosgw-admin zonegroup create --rgw-zonegroup=cn --rgw-realm=default --endpoints=https://ceph-rgw.example.internal --master --default

        # åˆ›å»ºä¸€ä¸ª Zoneï¼Œè®¾ä¸ºé»˜è®¤ä¸”ä¸ºä¸» Zone
        radosgw-admin zone create --rgw-zonegroup=cn --rgw-zone=cn-hangzhou --endpoints=https://ceph-rgw.example.internal --master --default

        # åˆ›å»ºä¸€ä¸ªç³»ç»ŸåŒæ­¥ç”¨æˆ·ï¼Œè®°å½•ç”Ÿæˆçš„ access_key å’Œ secret_key
        radosgw-admin user create --uid="synchronization-user" --display-name="Synchronization User" --system

        # å°†ç³»ç»Ÿç”¨æˆ·æ·»åŠ åˆ°ä¸» Zone
        radosgw-admin zone modify --rgw-zone=cn-hangzhou --access-key="ZRKBG2ODB20TPPR1X9GU" --secret="KErluNUPfiXR8LnZynePMDfyN6JLQG2sihNIzxnZ"

        # æäº¤ Period é…ç½®
        radosgw-admin period update --commit
        ```

    1. éƒ¨ç½² RGWï¼š

        ```bash
        ceph orch apply rgw rgw --realm=default --zonegroup=cn --zone=cn-hangzhou --placement="count:3" --port=8000
        ```

    1. Openresty ä»£ç†é…ç½®å¦‚ä¸‹ï¼š

        ```bash
        upstream ceph-rgw-backend {
            server 192.168.0.150:8000;
            server 192.168.0.151:8000;
            server 192.168.0.152:8000;
        }

        server {
            listen 443 ssl;
            server_name ceph-rgw.example.internal;

            include /etc/openresty/conf.d/ssl.conf;

            access_log      /var/log/openresty/ceph-rgw-access.log;
            error_log       /var/log/openresty/ceph-rgw-error.log;

            location / {
                client_max_body_size 10G;
                proxy_pass http://ceph-rgw-backend;

                proxy_connect_timeout 5s;
                proxy_send_timeout 30s;
                proxy_read_timeout 30s;

                proxy_set_header Connection $http_connection;
                proxy_set_header Upgrade $http_upgrade;
                proxy_set_header Host $host;
                proxy_set_header X-Real-IP $remote_addr;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
                proxy_set_header X-Forwarded-Proto $scheme;

                proxy_next_upstream error timeout http_503;
            }
        }
        ```

    1. å¯ç”¨ mgr ä¸­çš„ rgw æ¨¡å—ï¼š

        ```bash
        ceph mgr module enable rgw
        ```

    1. åˆ›å»ºä¸€ä¸ªç®¡ç†å‘˜ç”¨æˆ·ç”¨äº S3 æ¥å…¥ï¼š

        ```bash
        radosgw-admin user create --uid=admin --display-name="RGW Admin User" --email=admin@example.internal --admin
        ```

    1. åœ¨å®¢æˆ·ç«¯å®‰è£…å¹¶é…ç½® s3cmd å·¥å…·ï¼Œå®Œæˆå¯¹è±¡å­˜å‚¨çš„æµ‹è¯•æ•°æ®ä¸Šä¼ ï¼š

        ```bash
        s3cmd mb s3://test
        dd if=/dev/zero of=./test-64M.bin bs=1M count=64 status=progress
        s3cmd put test-64M.bin s3://test/
        ```

        ä¸Šä¼ æˆåŠŸåï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

        ```bash
        s3cmd info s3://test/test-64M.bin
        ```

        s3cmd ä¼šè¿”å›å¦‚ä¸‹å¯¹è±¡å…ƒæ•°æ®ä¿¡æ¯ï¼š

        ```bash
        s3://test/test-64M.bin (object):
        File size: 67108864
        Last mod:  Thu, 12 Jun 2025 16:13:35 GMT
        MIME type: application/octet-stream
        Storage:   STANDARD
        MD5 sum:   7f614da9329cd3aebf59b91aadc30bf0
        SSE:       none
        Policy:    none
        CORS:      none
        ACL:       RGW Admin User: FULL_CONTROL
        x-amz-meta-s3cmd-attrs: atime:1749744690/ctime:1749744535/gid:0/gname:root/md5:7f614da9329cd3aebf59b91aadc30bf0/mode:33188/mtime:1749744535/uid:0/uname:root
        ```

## ceph é›†ç¾¤çš„å¸è½½ä¸æ¸…é™¤

1. åœæ­¢é›†ç¾¤ç¼–æ’åŠŸèƒ½

    cephadm é»˜è®¤å¯ç”¨äº†é›†ç¾¤ç¼–æ’ï¼ˆorchestrationï¼‰åŠŸèƒ½ï¼Œä¸ºé¿å…æ‰§è¡Œè¿‡ç¨‹ä¸­æœ‰æ–°ç»„ä»¶è¢«é‡æ–°éƒ¨ç½²ï¼Œå¿…é¡»å…ˆç¦ç”¨ cephadm ç®¡ç†æ¨¡å—ï¼š

    ```bash
    ceph mgr module disable cephadm
    ```

1. ç¡®è®¤é›†ç¾¤ FSID

    ```bash
    ceph fsid
    ```

    ç¤ºä¾‹è¾“å‡ºå¦‚ä¸‹ï¼š

    ```bash
    1ec2ce44-1fff-11f0-a457-00505639e7ae
    ```

1. åœ¨é›†ç¾¤çš„*æ¯ä¸€å°ä¸»æœºä¸Š*ï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤æ¸…é™¤æ‰€æœ‰ ceph æœåŠ¡ã€æ•°æ®ç›˜åŠå…ƒæ•°æ®ï¼š

    ```bash
    cephadm rm-cluster --force --zap-osds --fsid 1ec2ce44-1fff-11f0-a457-00505639e7ae
    ```

    å‚æ•°è¯´æ˜ï¼š
   * `--force`ï¼šå¼ºåˆ¶ç§»é™¤ï¼Œä¸åšé¢å¤–ç¡®è®¤
   * `--zap-osds`ï¼šæ¸…ç©ºå¹¶æ ¼å¼åŒ–æ‰€æœ‰å·²éƒ¨ç½²çš„ OSD è®¾å¤‡ï¼ˆå½»åº•é”€æ¯æ•°æ®ï¼‰
   * `--fsid`ï¼šæŒ‡å®šè¦ç§»é™¤çš„é›†ç¾¤å”¯ä¸€ ID

1. è¿›ä¸€æ­¥æ‰§è¡Œä»¥ä¸‹æ“ä½œæ¸…ç†æ®‹ç•™æ–‡ä»¶å’Œä¾èµ–ï¼š

    ```bash
    rm -rf /etc/ceph/* /var/lib/ceph/* /var/log/ceph/*
    ```
