---
title: "åŸºäº cephadm éƒ¨ç½²é«˜å¯ç”¨ Ceph å­˜å‚¨é›†ç¾¤"
date: 2024-01-01T10:00:00+08:00
lastmod: 2025-06-07T10:00:00+08:00
description: "é€šè¿‡ cephadm å·¥å…·éƒ¨ç½²ä¼ä¸šçº§é«˜å¯ç”¨ Ceph é›†ç¾¤ï¼Œæ”¯æŒå¤šèŠ‚ç‚¹ç®¡ç†ã€è‡ªåŠ¨åŒ–è¿ç»´ã€Dashboard UIï¼Œé€‚ç”¨äºç”Ÿäº§ç¯å¢ƒã€‚"
categories: ["åŸºç¡€è®¾æ–½"]
tags: ["Ceph", "cephadm", "é«˜å¯ç”¨", "åˆ†å¸ƒå¼å­˜å‚¨", "ä¼ä¸šå®è·µ"]
series: ["éƒ¨ç½²æ–¹æ¡ˆ"]
keywords: ["cephadm é«˜å¯ç”¨", "Ceph å­˜å‚¨é›†ç¾¤", "Ceph è‡ªåŠ¨åŒ–éƒ¨ç½²"]
ShowToc: true
TocOpen: true
draft: true
---

## éƒ¨ç½²å·¥å…·é€‰å‹

Ceph æ˜¯ä¸€æ¬¾ä¼ä¸šçº§å¼€æºåˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿï¼Œå…·å¤‡é«˜å¯æ‰©å±•æ€§ä¸å¼ºä¸€è‡´æ€§ã€‚ç›®å‰ï¼Œceph-deploy å·²é€æ­¥è¢«æ·˜æ±°ï¼Œè€Œ ceph-ansible é¡¹ç›®è‡ª 2022 å¹´ 8 æœˆèµ·ä¹Ÿåœæ­¢å‘å¸ƒæ–°ç‰ˆæœ¬ï¼Œç¤¾åŒºå»ºè®®è¿ç§»è‡³ cephadmã€‚ä» Octopusï¼ˆ15.2.xï¼‰ç‰ˆæœ¬èµ·ï¼ŒCeph å®˜æ–¹å¼•å…¥å¹¶æ¨èä½¿ç”¨åŸºäºå®¹å™¨çš„éƒ¨ç½²ä¸è¿ç»´å·¥å…· cephadmï¼Œç”¨äºç»Ÿä¸€ç®¡ç†é›†ç¾¤çš„éƒ¨ç½²ã€å‡çº§ä¸æœåŠ¡ç¼–æ’ã€‚

åœ¨ Kubernetes ç¯å¢ƒä¸­ï¼ŒRook æä¾›åŸç”Ÿæ–¹å¼æ¥æ¶ˆè´¹ Ceph å­˜å‚¨ï¼Œæ˜¯å½“å‰æœ€ä¸»æµçš„é›†æˆæ–¹æ¡ˆã€‚Rook æ”¯æŒéƒ¨ç½²ç‹¬ç«‹ Ceph é›†ç¾¤ï¼Œä¹Ÿæ”¯æŒæ¥å…¥å¤–éƒ¨å·²éƒ¨ç½²çš„ Ceph é›†ç¾¤ï¼ˆExternal Cluster æ¨¡å¼ï¼‰ã€‚

ç„¶è€Œï¼ŒRook ç›®å‰å­˜åœ¨ä»¥ä¸‹å±€é™ï¼š

* Ceph åœ¨é«˜è´Ÿè½½æˆ–æ¢å¤åœºæ™¯ä¸‹å¯¹åº•å±‚ç¡¬ä»¶æ€§èƒ½å’Œç½‘ç»œå»¶è¿Ÿæ•æ„Ÿï¼ŒRook ä¾èµ–äº Kubernetes é›†ç¾¤æœ¬èº«çš„ç¨³å®šæ€§ï¼Œæ’éšœé“¾è·¯é•¿ï¼Œæç«¯åœºæ™¯ä¸‹å¯èƒ½éœ€è¦é¢å¤–è€ƒè™‘ K8s è°ƒåº¦ã€ç½‘ç»œã€Pod ç”Ÿå‘½å‘¨æœŸç­‰å¤šä¸ªç»´åº¦
* Rook è®¾è®¡ä¸Šæ›´é€‚åˆä¸€ä¸ª K8s é›†ç¾¤éƒ¨ç½²ä¸€å¥—ç‹¬ç«‹çš„ Ceph é›†ç¾¤ï¼Œè™½ç„¶è§£è€¦æ€§å¥½ï¼Œä½†æ˜¯å­˜å‚¨èµ„æºåœ¨é›†ç¾¤é—´éš¾ä»¥æœç”¨ï¼Œèµ„æºæˆæœ¬å’Œè¿ç»´å¤æ‚åº¦éƒ½ä¸å°
* Rook åªé€‚åˆäº‘åŸç”Ÿåœºæ™¯ï¼Œä¸å…¼å®¹è£¸é‡‘å±ã€ç§æœ‰äº‘ã€å…¬æœ‰äº‘ç­‰æ··åˆæ¶æ„ä¸‹ï¼Œéš¾ä»¥ä½œä¸ºç»Ÿä¸€ç®¡ç†çš„åº•å±‚å­˜å‚¨å¹³å°

æœ¬æ–‡é‡‡ç”¨ cephadm éƒ¨ç½² Ceph é›†ç¾¤ï¼ŒRook ä½œä¸ºæ¶ˆè´¹ç«¯æ¥å…¥çš„æ¶æ„ï¼Œå…¼é¡¾äº†è¿ç»´å¯æ§æ€§ä¸äº‘åŸç”Ÿç”Ÿæ€çš„é›†æˆèƒ½åŠ›ï¼Œé€‚ç”¨äºæ··åˆæ¶æ„ä¸‹çš„ç»Ÿä¸€å­˜å‚¨åœºæ™¯ã€‚å¯¹äº Dev/Test/Staging ç­‰éç”Ÿäº§ç¯å¢ƒï¼Œå¯å…±ç”¨ä¸€å¥—ç”± cephadm éƒ¨ç½²çš„ä¸­å°å‹é›†ç¾¤ï¼›å¯¹äº Prod ç¯å¢ƒï¼Œå»ºè®®ç‹¬ç«‹éƒ¨ç½²ä¸€å¥— Ceph é›†ç¾¤ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™ç§æ–¹å¼å¯ä»¥å¾ˆå¥½çš„é€‚é…æ··åˆæ¶æ„ï¼Œé¿å… Kubernetes æœ¬èº«çš„ä¸ç¨³å®šå½±å“ï¼Œå¹¶é™ä½é‡å¤éƒ¨ç½²æˆæœ¬ã€‚

## Ceph å¦‚ä½•ä½œä¸ºç»Ÿä¸€å­˜å‚¨å¹³å°

Ceph æ”¯æŒå—ï¼ˆRBDï¼‰ã€æ–‡ä»¶ï¼ˆCephFSï¼‰ã€å¯¹è±¡ï¼ˆRGWï¼‰ç­‰å¤šç§å­˜å‚¨åè®®ï¼š

| åè®®ç±»å‹        | åœºæ™¯                                |
| --------------- | ----------------------------------- |
| RBD å—å­˜å‚¨      | MySQLã€PostgreSQLã€MongoDB ç­‰æ•°æ®åº“ |
| CephFS æ–‡ä»¶å­˜å‚¨ | å…±äº«æŒ‚è½½ã€æ—¥å¿—å­˜å‚¨ã€NFS æœåŠ¡        |
| RGW å¯¹è±¡å­˜å‚¨    | S3ã€å¤‡ä»½ã€AI æ¨¡å‹ç®¡ç†               |

Ceph å¯ä½œä¸ºç»Ÿä¸€åº•å±‚å­˜å‚¨å¹³å°ï¼Œæ”¯æŒå®¹å™¨åº”ç”¨å¯¹å—ã€æ–‡ä»¶å’Œå¯¹è±¡å­˜å‚¨çš„å…¨é¢éœ€æ±‚ã€‚ç„¶è€Œï¼Œå¯¹äºæŸäº› I/O å»¶è¿Ÿæå…¶æ•æ„Ÿçš„åœºæ™¯ï¼ˆå¦‚ etcdã€Redisã€Kafka ç­‰ï¼‰ï¼Œå¯ä»¥è€ƒè™‘ä»¥ä¸‹å­˜å‚¨ç­–ç•¥ï¼š

* Local PV + StatefulSetï¼šé€šè¿‡é…ç½®èŠ‚ç‚¹äº²å’Œæ€§å¹¶ä½¿ç”¨èŠ‚ç‚¹æœ¬åœ°ç¡¬ç›˜ï¼ˆSSD/HDDï¼‰ï¼Œé€‚ç”¨äºå¯¹æ€§èƒ½æ•æ„Ÿçš„æœåŠ¡ï¼Œæä¾›é«˜ IOPS å’Œä½å»¶è¿Ÿã€‚
* K8s å¤–éƒ¨éƒ¨ç½²/äº‘æ‰˜ç®¡ï¼šè¿½æ±‚ K8s é›†ç¾¤æ— çŠ¶æ€ï¼Œå°†å­˜å‚¨æœåŠ¡ä¸ K8s å®Œå…¨è§£è€¦ï¼ŒKubernetes é›†ç¾¤å˜åŠ¨ã€è¿ç§»ã€å‡çº§æ—¶ï¼Œå­˜å‚¨æœåŠ¡å¯å®ç°æ— æ„ŸçŸ¥ï¼Œæé«˜å¯ç»´æŠ¤æ€§å’Œå¼¹æ€§ã€‚

## éƒ¨ç½²é«˜å¯ç”¨çš„ ceph é›†ç¾¤

ä¸‹é¢æˆ‘ä»¬å°†ä½¿ç”¨ cephadm åœ¨ HomeLab ä¸­æ¥å°è¯•æ„å»ºä¸€ä¸ªä¸‰èŠ‚ç‚¹çš„é«˜å¯ç”¨ Ceph é›†ç¾¤ã€‚

æ ¸å¿ƒç»„ä»¶ç‰ˆæœ¬ï¼š

```yaml
OS: Debian GNU/Linux 12ï¼ˆbookwormï¼‰
Podman: 4.3.1
Ceph: 19.2.2ï¼ˆsquidï¼‰
Kubernetes: 1.31.7
Rook: v1.17.0
```

ğŸ“Œæ³¨ï¼šå¼ºçƒˆæ¨èä½¿ç”¨ Podman ä»£æ›¿ Dockerï¼ŒPodman æ”¯æŒ rootless æ¨¡å¼ã€æ— å®ˆæŠ¤è¿›ç¨‹ï¼ˆdaemonlessï¼‰è¿è¡Œï¼Œä¸”å¯åŸç”Ÿé›†æˆ systemd å•å…ƒæœåŠ¡ï¼Œç³»ç»Ÿäº²å’Œæ€§æ›´å¼ºã€‚ç”±äº Red Hat åŒæ—¶æ˜¯ Ceph ä¸ Podman çš„ä¸»è¦ç»´æŠ¤æ–¹ï¼Œä½¿ç”¨ Podman èƒ½è·å¾—æ›´å¥½çš„å…¼å®¹æ€§ã€å®‰å…¨æ€§ä¸å®˜æ–¹æ”¯æŒä¿éšœã€‚

éƒ¨ç½²å‰ç¡®è®¤ä»¥ä¸‹äº‹é¡¹ï¼š

* ä¸‰å°è£¸æœºæˆ–è™šæ‹Ÿæœºï¼ˆè¿™é‡Œä»¥ 192.168.0.{150..152} ä¸‰å°ä¸ºä¾‹ï¼‰
* æ¯ä¸ªèŠ‚ç‚¹è‡³å°‘ä¸º Ceph å‡†å¤‡ä¸€å—æ•°æ®ç›˜ï¼ˆæ— éœ€æ ¼å¼åŒ–ï¼‰
* æ“ä½œç³»ç»Ÿæ”¯æŒ Python3ã€Systemdã€Podman/Dockerã€LVM2ï¼ˆç›®å‰ä¸»æµæ“ä½œç³»ç»ŸåŸºæœ¬éƒ½æ”¯æŒï¼‰
* å·²è¿›è¡Œæ—¶é—´åŒæ­¥

å…·ä½“éƒ¨ç½²æµç¨‹ï¼š

1. ssh åˆ°ä¸»æ§èŠ‚ç‚¹ï¼Œç›´æ¥å®‰è£… cephadmï¼ˆä¹Ÿå¯ä»¥ä½¿ç”¨ curl æ–¹å¼ï¼‰ï¼Œä¸ç”¨æ‹…å¿ƒå‘è¡Œç‰ˆä¸­ cephadm ç‰ˆæœ¬è¿‡æ—§çš„é—®é¢˜ï¼Œè¿™é‡Œåªæ˜¯ä¸ºäº†é€šè¿‡è¯¥å‘½ä»¤é…ç½®ä»“åº“ã€‚

    ```bash
    apt install -y cephadm
    ```

1. é€šè¿‡ cephadm æ·»åŠ  Ceph å®˜æ–¹æœ€æ–°ç¨³å®šç‰ˆä»“åº“ï¼Œå¹¶å®‰è£…æœ€æ–°çš„ cephadm ä¾èµ–åŒ…

    ```bash
    cephadm add-repo --release squid
    cephadm install
    ```

1. æ£€æŸ¥ PATH å˜é‡

    ```bash
    which cephadm
    ```

    æˆåŠŸå°†è¿”å›ä»¥ä¸‹ä¿¡æ¯ï¼š

    ```bash
    /usr/sbin/cephadm
    ```

1. å¼•å¯¼åˆ›å»ºæ–°é›†ç¾¤

    ```bash
    cephadm bootstrap --mon-ip 192.168.0.150
    ```

    è¯¥æ¡å‘½ä»¤åšäº†ä»¥ä¸‹å‡ ä»¶äº‹ï¼š

    * åœ¨æœ¬åœ°ä¸»æœºä¸Šåˆ›å»º MON å’Œ MGR å®ˆæŠ¤è¿›ç¨‹
    * ç”Ÿæˆé›†ç¾¤å¯†é’¥
    * é…ç½®äº†æœ€å°åŒ–çš„ ceph.conf æ–‡ä»¶åŠ admin å¯†é’¥æ–‡ä»¶
    * å°†å½“å‰ä¸»æœºæ‰“ä¸Š _admin æ ‡ç­¾ä»¥ä¾¿ç®¡ç†è®¿é—®

    è¿™é‡Œé¢å¤–ç»™å‡ºå‡ ä¸ªå¸¸ç”¨çš„å‚æ•°ï¼Œå¯æ‰§è¡Œ `cephadm bootstrap -h` æŸ¥çœ‹æ›´å¤šå‚æ•°ï¼š
    * `--allow-fqdn-hostname`ï¼šå…è®¸ä½¿ç”¨ FQDN ä½œä¸ºä¸»æœºå
    * `--log-to-file`ï¼šè¾“å‡ºæ—¥å¿—åˆ°æœ¬åœ°æ–‡ä»¶
    * `--public-network`ï¼šæŒ‡å®šå®¢æˆ·ç«¯ä¸ MONã€OSDã€MDSã€RGW ç­‰ Ceph ç»„ä»¶é€šä¿¡çš„ç½‘ç»œ
    * `--cluster-network`ï¼šæŒ‡å®š OSD èŠ‚ç‚¹é—´ä¸“ç”¨çš„å†…éƒ¨ç½‘ç»œï¼Œç”¨äºæ•°æ®å¤åˆ¶ã€æ¢å¤å’Œå¿ƒè·³æ£€æµ‹
    * `--config`ï¼šæŒ‡å®šè‡ªå®šä¹‰çš„é…ç½®æ–‡ä»¶è·¯å¾„

1. å¯ç”¨ ceph cli

    è¦æ‰§è¡Œ ceph å‘½ä»¤ï¼Œéœ€è¦é€šè¿‡ ceph shell çš„æ–¹å¼åˆ°å®¹å™¨ä¸­è¿è¡Œï¼š

    ```bash
    cephadm shell -- ceph -s
    ```

    ä¸ºäº†æ–¹ä¾¿è¿ç»´ï¼Œå»ºè®®å®‰è£… ceph-common è½¯ä»¶åŒ…ï¼Œé‡Œé¢åŒ…å«äº†æ‰€æœ‰ ceph å‘½ä»¤ï¼ŒåŒ…æ‹¬ cephã€rbdã€mount.cephï¼ˆç”¨äºæŒ‚è½½CephFSæ–‡ä»¶ç³»ç»Ÿï¼‰ç­‰ï¼š

    ```bash
    cephadm install ceph-common
    ```

    æ¥ä¸‹æ¥ï¼Œä¾¿å¯ä»¥ç›´æ¥ä½¿ç”¨ ceph å‘½ä»¤äº†ï¼Œæ¯”å¦‚æŸ¥çœ‹ ceph é›†ç¾¤çŠ¶æ€ï¼š

    ```bash
    ceph status
    ```

    åº”è¯¥ä¼šçœ‹åˆ°ä¸€ä¸ª mon å’Œ ä¸€ä¸ª mgr è¿›ç¨‹ï¼š

    ```bash
    cluster:
        id:     852837fa-46a1-11f0-a6ed-00505639e7ae
        health: HEALTH_WARN
                OSD count 0 < osd_pool_default_size 3
    
    services:
        mon: 1 daemons, quorum kube1 (age 2m)
        mgr: kube1.iuiljm(active, since 54s)
        osd: 0 osds: 0 up, 0 in
    
    data:
        pools:   0 pools, 0 pgs
        objects: 0 objects, 0 B
        usage:   0 B used, 0 B / 0 B avail
        pgs:   
    ```

1. æ·»åŠ å…¶ä»–ä¸»æœºåˆ°é›†ç¾¤ä¸­ï¼š

    å°†é›†ç¾¤çš„å…¬å…± SSH å¯†é’¥å®‰è£…åˆ°æ–°ä¸»æœº root ç”¨æˆ·çš„ authorized_keys æ–‡ä»¶ä¸­ï¼š
  
    ```bash
    ssh-copy-id -f -i /etc/ceph/ceph.pub root@kube2
    ssh-copy-id -f -i /etc/ceph/ceph.pub root@kube3
    ```

    é€šçŸ¥ Ceph æœ‰æ–°èŠ‚ç‚¹åŠ å…¥é›†ç¾¤ï¼š

    ```bash
    ceph orch host add kube2 192.168.0.151 --labels _admin
    ceph orch host add kube3 192.168.0.152 --labels _admin
    ```

    ğŸ“Œæ³¨ï¼šé»˜è®¤æƒ…å†µä¸‹ï¼Œ_admin æ ‡ç­¾ä¼šè®© cephadm åœ¨è¯¥ä¸»æœºçš„ /etc/ceph ç›®å½•ä¸‹ç»´æŠ¤ä¸€ä»½ ceph.conf é…ç½®æ–‡ä»¶å’Œ client.admin å¯†é’¥ç¯æ–‡ä»¶ï¼Œé€‚ç”¨äºéƒ¨ç½² MONã€MGR ç­‰å…³é”®æœåŠ¡èŠ‚ç‚¹ã€‚

    å¯ä»¥é€šè¿‡æ­¤å‘½ä»¤æŸ¥çœ‹å„èŠ‚ç‚¹æœåŠ¡åˆ—è¡¨ï¼š

    ```bash
    ceph orch ls
    ```

    å¦‚æœå‘ç°è¾“å‡ºç»“æœæœ‰ç‚¹ä¸ç¬¦é¢„æœŸï¼Œé‚£æ˜¯å› ä¸º `cephadm boostrap` é»˜è®¤ç­–ç•¥ä¸‹ mon æ•°é‡ä¸º 5ï¼Œç”±äºæˆ‘è¿™é‡Œåªæœ‰ 3 ä¸ªèŠ‚ç‚¹ï¼Œæ‰€ä»¥éœ€è¦æ‰‹åŠ¨å°† mon æ•°é‡è°ƒæ•´æˆ 3ï¼š

    ```bash
    ceph orch apply mon count:3
    ```

    è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

    ```bash
    NAME           PORTS        RUNNING  REFRESHED  AGE  PLACEMENT  
    alertmanager   ?:9093,9094      1/1  30s ago    5m   count:1    
    ceph-exporter                   3/3  30s ago    5m   *          
    crash                           3/3  30s ago    6m   *          
    grafana        ?:3000           1/1  30s ago    5m   count:1    
    mgr                             2/2  30s ago    6m   count:2    
    mon                             3/3  30s ago    1s   count:3    
    node-exporter  ?:9100           3/3  30s ago    5m   *          
    prometheus     ?:9095           1/1  30s ago    5m   count:1    
    ```

1. æ·»åŠ  OSD å­˜å‚¨ï¼š

    1. æ£€æŸ¥æ‰€æœ‰é›†ç¾¤ä¸»æœºä¸Šçš„å­˜å‚¨è®¾å¤‡ä¿¡æ¯ï¼š

        ```bash
        ceph orch device ls
        ```

        å­˜å‚¨è®¾å¤‡å¯ç”¨çš„å‰ææ˜¯ï¼š
        * è®¾å¤‡ä¸Šä¸èƒ½æœ‰ä»»ä½•åˆ†åŒº
        * è®¾å¤‡ä¸Šä¸èƒ½å­˜åœ¨ä»»ä½• LVM çŠ¶æ€
        * è®¾å¤‡ä¸èƒ½è¢«æŒ‚è½½
        * è®¾å¤‡ä¸Šä¸èƒ½åŒ…å«æ–‡ä»¶ç³»ç»Ÿ
        * è®¾å¤‡ä¸Šä¸èƒ½åŒ…å« Ceph BlueStore OSD
        * è®¾å¤‡å®¹é‡å¿…é¡»å¤§äº 5 GB
        * Ceph ä¸ä¼šåœ¨ä¸ç¬¦åˆæ¡ä»¶çš„è®¾å¤‡ä¸Šåˆ›å»º OSD

    1. æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ä»¥é¢„æ¼”é…ç½®å˜æ›´ï¼ˆå¯èƒ½éœ€è¦æ‰§è¡Œä¸¤æ¬¡ä¹Ÿä¼šæ˜¾ç¤ºç»“æœï¼‰ï¼š

        ```bash
        ceph orch apply osd --all-available-devices --dry-run
        ```

        ğŸ“Œæ³¨ï¼š`ceph orch apply` ä¼šè®© cephadm æŒç»­å¯¹çŠ¶æ€è¿›è¡Œåè°ƒï¼ˆreconcileï¼‰ï¼Œä¿è¯é›†ç¾¤çŠ¶æ€ä¸æœŸæœ›é…ç½®ä¸€è‡´ã€‚
        æ¢å¥è¯è¯´ï¼ŒCeph é›†ç¾¤ä¼šæŒç»­ç›‘æ§å’Œè‡ªåŠ¨ç®¡ç†ç¬¦åˆæ¡ä»¶çš„è®¾å¤‡ï¼š
          * æ–°åŠ å…¥é›†ç¾¤çš„ç£ç›˜ä¼šè¢«è‡ªåŠ¨å‘ç°å¹¶ç”¨äºåˆ›å»ºæ–°çš„ OSD
          * å¦‚æœæŸä¸ª OSD è¢«ç§»é™¤ä¸”å¯¹åº”çš„ LVM ç‰©ç†å·è¢«æ¸…ç†ï¼ˆzapï¼‰ï¼ŒCeph ä¹Ÿä¼šè‡ªåŠ¨åœ¨è¯¥è®¾å¤‡ä¸Šé‡æ–°åˆ›å»ºæ–°çš„ OSD

    1. ç»§ç»­æ‰§è¡Œå®Œæˆ osd åˆ›å»ºï¼Œæ•æ„Ÿç¯å¢ƒä¸­å¯ä»¥è¿½åŠ  `--unmanaged` å‚æ•°æ”¹ç”¨æ‰‹åŠ¨ç®¡ç†ä»¥é¿å…è¯¯æ“ä½œï¼š

        ```bash
        ceph orch apply osd --all-available-devices
        ```

    1. æˆ–è€…ä½ ä¹Ÿå¯ä»¥æ‰‹åŠ¨æŒ‡å®šï¼Œè¿™æ ·æ›´å®‰å…¨å¯æ§ï¼ˆå¯é€‰ï¼‰

        ```bash
        ceph orch daemon add osd *<host>*:*<device-path>*
        ```

    1. ç¨å¾®ä¸€ä¼šï¼Œæ£€æŸ¥é›†ç¾¤çŠ¶æ€ï¼Œæ˜¾ç¤ºå·²æœ‰ 3 ä¸ª osd åŠ å…¥åˆ°é›†ç¾¤ä¸­ï¼Œä¸”ä¸º up çŠ¶æ€ï¼š

        ```bash
        cluster:
            id:     852837fa-46a1-11f0-a6ed-00505639e7ae
            health: HEALTH_OK
        
        services:
            mon: 3 daemons, quorum kube1,kube2,kube3 (age 22m)
            mgr: kube1.iuiljm(active, since 24m), standbys: kube2.mpdvnm
            osd: 3 osds: 3 up (since 68s), 3 in (since 2m)
        
        data:
            pools:   1 pools, 1 pgs
            objects: 2 objects, 449 KiB
            usage:   81 MiB used, 30 GiB / 30 GiB avail
            pgs:     1 active+clean  
        ```

1. å¼€å¯å†…å­˜è‡ªåŠ¨è°ƒèŠ‚é€‰é¡¹

    cephadm å¼•å¯¼é›†ç¾¤æ—¶é»˜è®¤ä¼šå¯ç”¨ `osd_memory_target_autotune = true`ï¼Œå¹¶è®¾ç½® `mgr/cephadm/autotune_memory_target_ratio = 0.7`ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯å°ä¸»æœºçš„ OSD é»˜è®¤æœ€å¤šä½¿ç”¨è¯¥èŠ‚ç‚¹æ€»å†…å­˜çš„ 70%ï¼Œç”¨äº BlueStore çš„ç¼“å­˜ï¼Œå½“ä½ çš„å­˜å‚¨é›†ç¾¤ä¸Šæœ‰å…¶ä»–æœåŠ¡æ—¶ï¼Œå¯å»ºè®®æŒ‰éœ€è°ƒå°è¯¥å‚æ•°ï¼Œé˜²æ­¢ OOMã€‚

    ```bash
    ceph config set osd osd_memory_target_autotune true
    ceph config set mgr mgr/cephadm/autotune_memory_target_ratio 0.3
    ```

1. å¼€å¯ dashboard

    1. é€šè¿‡ä»¥ä¸‹å‘½ä»¤å¯ç”¨ dashboardï¼š

    ```bash
    ceph mgr module enable dashboard
    ```

    1. è®¾ç½® SSL/TLS æ”¯æŒï¼Œè¿™é‡Œä»…ä½¿ç”¨è‡ªç­¾åè¯ä¹¦ï¼š

    ```bash
    ceph dashboard create-self-signed-cert
    ```

    1. åˆ›å»ºä¸€ä¸ªç®¡ç†å‘˜ç”¨æˆ· adminï¼Œè¿™é‡Œä½¿ç”¨ `--force-password` ç»•è¿‡å¯†ç ç­–ç•¥æ£€æŸ¥ï¼Œå¹¶ä½¿ç”¨ `--pwd_update_required` ç¡®ä¿é¦–æ¬¡ç™»å½•åå¼ºåˆ¶è¦æ±‚ç”¨æˆ·æ›´æ”¹å¯†ç ï¼š

    ```bash
    echo "123456" > admin.pass
    ceph dashboard ac-user-create --force-password --pwd_update_required admin -i admin.pass administrator
    ```

    1. ç›®å‰æˆ‘ä»¬çš„é›†ç¾¤é‡Œæœ‰ä¸¤ä¸ª mgrï¼Œä¸ºäº†æé«˜å¯ç”¨æ€§ï¼Œæˆ‘ä»¬éœ€è¦å¼•å…¥ä»£ç†æœåŠ¡ï¼Œå»ºè®®å…ˆæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

    ```bash
    ceph config set mgr mgr/dashboard/standby_behaviour "error"
    ceph config set mgr mgr/dashboard/standby_error_status_code 503
    ```

    ğŸ“Œæ³¨ï¼šä¸Šè¿°å‘½ä»¤ä¼šè®©é active çš„ mgr dashboard ä¸å“åº” 303 Redirectï¼Œè€Œæ˜¯è‡ªå®šä¹‰ç›¸åº” 503 çŠ¶æ€ç ï¼Œå¦åˆ™ä»£ç†æœåŠ¡åœ¨ dashboard failover åå°†æ”¶åˆ°é”™è¯¯è·³è½¬åœ°å€ã€‚

    1. å®˜æ–¹ç»™çš„æ˜¯ haproxy ç¤ºä¾‹ï¼Œæˆ‘çš„ç¯å¢ƒä»…éƒ¨ç½²äº† openrestyï¼Œå› æ­¤è¿™é‡Œç»™å‡º openresty çš„åŸºæœ¬é…ç½®ï¼Œå¦‚æœä¸¤ä¸ªèŠ‚ç‚¹ä»¥ä¸Šå»ºè®®å¼•å…¥ health check å®ç°åŠ¨æ€ upstreamï¼š

    ```bash
    upstream ceph-dashboard-backend {
        server 192.168.0.150:8443;
        server 192.168.0.151:8443;
    }

    server {
        listen 443 ssl;
        server_name ceph-dashboard.example.internal;

        include /etc/openresty/conf.d/ssl.conf;

        access_log /var/log/openresty/ceph-dashboard-access.log;
        error_log  /var/log/openresty/ceph-dashboard-error.log;

        location / {
            proxy_pass https://ceph-dashboard-backend;

            proxy_ssl_server_name on;
            proxy_ssl_verify off;

            proxy_connect_timeout 5s;
            proxy_send_timeout 30s;
            proxy_read_timeout 30s;

            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;

            proxy_next_upstream error timeout http_503;
        }
    }
    ```

1. ä½¿ç”¨ CephFS

    è¦ä½¿ç”¨ CephFS æ–‡ä»¶ç³»ç»Ÿï¼Œéœ€è¦ä¸€ä¸ªæˆ–å¤šä¸ª MDSï¼ˆMetadata Serverï¼‰å®ˆæŠ¤è¿›ç¨‹ã€‚å¦‚æœä½¿ç”¨è¾ƒæ–°çš„ `ceph fs volume` æ¥å£æ¥åˆ›å»ºæ–°çš„æ–‡ä»¶ç³»ç»Ÿï¼Œè¿™äº›å®ˆæŠ¤è¿›ç¨‹ä¼šè‡ªåŠ¨åˆ›å»ºã€‚

    1. åˆ›å»º CephFS å·

        ```bash
        ceph fs volume create cephfs --placement="count=2"
        ```

        * `cephfs`ï¼šä¸º CephFS æ–‡ä»¶ç³»ç»ŸæŒ‡å®šçš„åç§°
        * `--placement="count=2"`ï¼šå‘Šè¯‰ cephadm åœ¨é›†ç¾¤ä¸­éƒ¨ç½² 2 ä¸ª MDS å®ˆæŠ¤è¿›ç¨‹ï¼ˆ1 active, 1 standbyï¼‰ï¼Œcephadm ä¼šè‡ªåŠ¨é€‰æ‹©åˆé€‚çš„èŠ‚ç‚¹æ¥éƒ¨ç½²è¿™äº›å®ˆæŠ¤è¿›ç¨‹

    1. éƒ¨ç½² MDS

        ```bash
        ceph orch apply mds cephfs 'kube[1-3]'
        ```

    1. éªŒè¯ cephfs çŠ¶æ€

        ```bash
        ceph fs status
        ```

        é»˜è®¤æƒ…å†µä¸‹ä¼šæœ‰ä¸€å°ä¸º activeï¼Œå…¶ä½™èŠ‚ç‚¹å‡ä¸º standbyï¼š

        ```bash
        cephfs - 0 clients
        ======
        RANK  STATE           MDS             ACTIVITY     DNS    INOS   DIRS   CAPS  
        0    active  cephfs.kube2.tekede  Reqs:    0 /s    10     13     12      0   
            POOL           TYPE     USED  AVAIL  
        cephfs.cephfs.meta  metadata  96.0k  9697M  
        cephfs.cephfs.data    data       0   9697M  
            STANDBY MDS      
        cephfs.kube1.kjracz  
        cephfs.kube3.rdjvxy  
        MDS version: ceph version 19.2.2 (0eceb0defba60152a8182f7bd87d164b639885b8) squid (stable)
        ```

1. æ”¯æŒ NFS

1. æ”¯æŒ RGWs

## ceph é›†ç¾¤çš„å¸è½½ä¸æ¸…é™¤

1. åœæ­¢é›†ç¾¤ç¼–æ’åŠŸèƒ½

    cephadm é»˜è®¤å¯ç”¨äº†é›†ç¾¤ç¼–æ’ï¼ˆorchestrationï¼‰åŠŸèƒ½ï¼Œä¸ºé¿å…æ‰§è¡Œè¿‡ç¨‹ä¸­æœ‰æ–°ç»„ä»¶è¢«é‡æ–°éƒ¨ç½²ï¼Œå¿…é¡»å…ˆç¦ç”¨ cephadm ç®¡ç†æ¨¡å—ï¼š

    ```bash
    ceph mgr module disable cephadm
    ```

1. ç¡®è®¤é›†ç¾¤ FSID

    ```bash
    ceph fsid
    ```

    ç¤ºä¾‹è¾“å‡ºå¦‚ä¸‹ï¼š

    ```bash
    1ec2ce44-1fff-11f0-a457-00505639e7ae
    ```

1. åœ¨é›†ç¾¤çš„*æ¯ä¸€å°ä¸»æœºä¸Š*ï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤æ¸…é™¤æ‰€æœ‰ ceph æœåŠ¡ã€æ•°æ®ç›˜åŠå…ƒæ•°æ®ï¼š

    ```bash
    cephadm rm-cluster --force --zap-osds --fsid 1ec2ce44-1fff-11f0-a457-00505639e7ae
    ```

    å‚æ•°è¯´æ˜ï¼š
   * `--force`ï¼šå¼ºåˆ¶ç§»é™¤ï¼Œä¸åšé¢å¤–ç¡®è®¤
   * `--zap-osds`ï¼šæ¸…ç©ºå¹¶æ ¼å¼åŒ–æ‰€æœ‰å·²éƒ¨ç½²çš„ OSD è®¾å¤‡ï¼ˆå½»åº•é”€æ¯æ•°æ®ï¼‰
   * `--fsid`ï¼šæŒ‡å®šè¦ç§»é™¤çš„é›†ç¾¤å”¯ä¸€ ID

1. è¿›ä¸€æ­¥æ‰§è¡Œä»¥ä¸‹æ“ä½œæ¸…ç†æ®‹ç•™æ–‡ä»¶å’Œä¾èµ–ï¼š

    ```bash
    rm -rf /etc/ceph/* /var/lib/ceph/* /var/log/ceph/*
    ```
